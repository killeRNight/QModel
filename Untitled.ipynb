{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import h5py\n",
    "import helper as hl\n",
    "from sklearn.externals import joblib\n",
    "from tensorflow.python.keras.layers import Input, GRU, Dense, Concatenate, TimeDistributed\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.layers import Layer\n",
    "from tensorflow.python.keras import backend as T\n",
    "import configparser\n",
    "import helper as hl\n",
    "import tensorflow as tf\n",
    "from tensorflow import set_random_seed\n",
    "import warnings\n",
    "np.random.seed(27)\n",
    "set_random_seed(27)\n",
    "pd.set_option('display.max_colwidth', 1500)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "pd.set_option('display.width', 900)\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(Layer):\n",
    "    \"\"\"\n",
    "    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n",
    "    There are three sets of weights introduced W_a, U_a, and V_a\n",
    "     \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert isinstance(input_shape, list)\n",
    "        # Create a trainable weight variable for this layer.\n",
    "\n",
    "        self.W_a = self.add_weight(name='W_a',\n",
    "                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "        self.U_a = self.add_weight(name='U_a',\n",
    "                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "        self.V_a = self.add_weight(name='V_a',\n",
    "                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "        \n",
    "        # Be sure to call this at the end\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, verbose=False):\n",
    "        \n",
    "        \"\"\"\n",
    "        inputs: [encoder_output_sequence, decoder_output_sequence]\n",
    "        \"\"\"\n",
    "        \n",
    "        assert type(inputs) == list\n",
    "        encoder_out_seq, decoder_out_seq = inputs\n",
    "        if verbose:\n",
    "            print('encoder_out_seq>', encoder_out_seq.shape)\n",
    "            print('decoder_out_seq>', decoder_out_seq.shape)\n",
    "\n",
    "        def energy_step(inputs, states):\n",
    "            \"\"\" Step function for computing energy for a single decoder state \"\"\"\n",
    "\n",
    "            assert_msg = \"States must be a list. However states {} is of type {}\".format(states, type(states))\n",
    "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
    "\n",
    "            \"\"\" Some parameters required for shaping tensors\"\"\"\n",
    "            batch_size = encoder_out_seq.shape[0]\n",
    "            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n",
    "            de_hidden = inputs.shape[-1]\n",
    "\n",
    "            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n",
    "            # <= batch_size*en_seq_len, latent_dim\n",
    "            reshaped_enc_outputs = T.reshape(encoder_out_seq, (batch_size * en_seq_len, en_hidden))\n",
    "            # <= batch_size*en_seq_len, latent_dim\n",
    "            W_a_dot_s = T.reshape(T.dot(reshaped_enc_outputs, self.W_a), (batch_size, en_seq_len, en_hidden))\n",
    "            if verbose:\n",
    "                print('wa.s>',W_a_dot_s.shape)\n",
    "\n",
    "            \"\"\" Computing hj.Ua \"\"\"\n",
    "            U_a_dot_h = T.expand_dims(T.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n",
    "            if verbose:\n",
    "                print('Ua.h>',U_a_dot_h.shape)\n",
    "\n",
    "            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n",
    "            # <= batch_size*en_seq_len, latent_dim\n",
    "            reshaped_Ws_plus_Uh = T.tanh(T.reshape(W_a_dot_s + U_a_dot_h, (batch_size * en_seq_len, en_hidden)))\n",
    "            if verbose:\n",
    "                print('Ws+Uh>', reshaped_Ws_plus_Uh.shape)\n",
    "\n",
    "            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n",
    "            # <= batch_size, en_seq_len\n",
    "            e_i = T.reshape(T.dot(reshaped_Ws_plus_Uh, self.V_a), (batch_size, en_seq_len))\n",
    "            # <= batch_size, en_seq_len\n",
    "            e_i = T.softmax(e_i)\n",
    "\n",
    "            if verbose:\n",
    "                print('ei>', e_i.shape)\n",
    "\n",
    "            return e_i, [e_i]\n",
    "\n",
    "        def context_step(inputs, states):\n",
    "            \"\"\" Step function for computing ci using ei \"\"\"\n",
    "            # <= batch_size, hidden_size\n",
    "            c_i = T.sum(encoder_out_seq * T.expand_dims(inputs, -1), axis=1)\n",
    "            if verbose:\n",
    "                print('ci>', c_i.shape)\n",
    "            return c_i, [c_i]\n",
    "\n",
    "        # We are not using initial states, but need to pass something to T.rnn funciton\n",
    "        fake_state_c = T.zeros(shape=(encoder_out_seq.shape[0], encoder_out_seq.shape[-1]))\n",
    "        fake_state_e = T.zeros(shape=(encoder_out_seq.shape[0], encoder_out_seq.shape[1]))\n",
    "\n",
    "        \"\"\" \n",
    "        Computing energy outputs\n",
    "        \"\"\"\n",
    "        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n",
    "        last_out, e_outputs, _ = T.rnn(\n",
    "            energy_step, decoder_out_seq, [fake_state_e],\n",
    "        )\n",
    "\n",
    "        \"\"\"\n",
    "        Computing context vectors\n",
    "        \"\"\"\n",
    "        last_out, c_outputs, _ = T.rnn(\n",
    "            context_step, e_outputs, [fake_state_c],\n",
    "        )\n",
    "\n",
    "        return c_outputs, e_outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \"\"\" Outputs produced by the layer \"\"\"\n",
    "        return [\n",
    "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n",
    "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fit():\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        # Path to options file.\n",
    "        self.path_options = 'options.ini'\n",
    "\n",
    "        # Load config file.\n",
    "        config = configparser.ConfigParser()                                             # define config object.\n",
    "        config.optionxform = str                                                         # hold keys register\n",
    "        config.read(self.path_options)                                                   # read config file.\n",
    "\n",
    "        # Path variables.\n",
    "        self.path_data = config['PATH'].get('DataObj')                                   # address: data object\n",
    "        self.path_alphas = config['PATH'].get('Alphas')                                  # address: alphas\n",
    "        self.path_events = config['PATH'].get('Events')                                  # address: events\n",
    "        self.path_tensors = config['PATH'].get('Tensors')                                # address: tensors\n",
    "        self.path_positions = config['PATH'].get('PositionsObj')                         # address: predictions\n",
    "\n",
    "        # Load data object\n",
    "        self.data = joblib.load(self.path_data)                                          # data object\n",
    "\n",
    "        # Internal variables.\n",
    "        self.alpha_storage = None                                                        # alpha model storage\n",
    "        self.beta_storage = None                                                         # beta model storage\n",
    "        self.selected_alphas = None                                                      # selected alphas\n",
    "        self.start_transfer_learning = False                                             # start tf mark\n",
    "        self.freeze_control = 0                                                          # layer freeze control\n",
    "\n",
    "    # Fn: (1)\n",
    "    # Model is trained on each refit day and\n",
    "    # then predictions are made.\n",
    "    def run_model(self):\n",
    "\n",
    "        # Load config file.\n",
    "        config = configparser.ConfigParser()                                             # define config object.\n",
    "        config.optionxform = str                                                         # hold keys register\n",
    "        config.read(self.path_options)                                                   # read config file.\n",
    "\n",
    "        # Create positions folder is it doesn`t exist.\n",
    "        if not os.path.exists(self.path_positions.replace('positions.pickle', '')):\n",
    "            os.makedirs(self.path_positions.replace('positions.pickle', ''))\n",
    "\n",
    "        print('')\n",
    "        print('Fit model:')\n",
    "        \"\"\"        \n",
    "        ----------------------------------------------------------------------------------------------------------                                                     \n",
    "                                                         DATA\n",
    "                                        Load all necessary data variables and \n",
    "                                                    model parameters.                                                                                                          \n",
    "        ----------------------------------------------------------------------------------------------------------\n",
    "        \"\"\"\n",
    "        # ---------------------------\n",
    "        #            DATA\n",
    "        # ----------------------------\n",
    "        # Dates. arr.\n",
    "        dates = self.data['dates']\n",
    "        # Industry data. arr.\n",
    "        industry = self.data['industry.sector']\n",
    "        # Selected alphas. arr.\n",
    "        alphas_list = os.listdir(self.path_alphas)\n",
    "        events_list = os.listdir(self.path_events)\n",
    "        # Data holders sizes. val.\n",
    "        numstocks = self.data['numstocks']\n",
    "        numdates = self.data['numdates']\n",
    "        print('Days available: {}'.format(len(dates)))\n",
    "\n",
    "        # -------------------------------\n",
    "        #          PARAMETERS\n",
    "        # -------------------------------\n",
    "\n",
    "        # General\n",
    "        lookback = 200                                # days for training\n",
    "        fit_startdate = config['DEFAULT'].getint('FitStartDate')                        # {20100104: 20180205}\n",
    "        delay = config['DEFAULT'].getint('Delay')                                       # delay days\n",
    "        refit_freq = config['DEFAULT'].getint('RefitFreq')                              # refit frequency\n",
    "\n",
    "        # Target value\n",
    "        ndays = config['FIT'].getint('TargetNDays')                                     # return mean value\n",
    "        ret_cap = config['FIT'].getfloat('TargetRetCap')                                # return cap.\n",
    "        min_target = config['FIT'].getfloat('TargetMinValOr')                           # target values min cap\n",
    "        min_target_z = config['FIT'].getfloat('TargetMinValZc')                         # target values max cap\n",
    "        industry_balancing = config['FIT'].getboolean('TargetIndBalancing')             # perform industry balancing\n",
    "\n",
    "        # Alpha adjustment\n",
    "        min_alpha = config['FIT'].getfloat('AlphaMin')                                  # alpha values min cap\n",
    "        max_alpha = config['FIT'].getfloat('AlphaMax')                                  # alpha values max cap\n",
    "\n",
    "        # Model\n",
    "        loss_adj = config['FIT'].getfloat('ModelLossAdj')                               # stock loss fn. adj\n",
    "        reg_ffn = config['FIT'].getfloat('ModelRegFFN')                                 # reg. ffn\n",
    "        reg_conv = config['FIT'].getfloat('ModelRegConvNet')                            # reg. con\n",
    "        reg_gru = config['FIT'].getfloat('ModelRegGRU')                                 # reg. rnn\n",
    "        layer_capacity = config['FIT'].getint('ModelLayerCapacity')                     # neurons n. resnet\n",
    "        n_layer_1 = config['FIT'].getint('ModelNeuronsLayer1')                          # neurons n. 1-st layer\n",
    "        n_layer_2 = config['FIT'].getint('ModelNeuronsLayer2')                          # neurons n. 2-nd layer\n",
    "        deep_features_n = config['FIT'].getint('ModelDeepFeaturesNumber')               # deep features number\n",
    "        ch_layer_1 = config['FIT'].getint('ModelChannelLayer1')                         # channels n. 1-st layer\n",
    "        ch_layer_2 = config['FIT'].getint('ModelChannelLayer2')                         # channels n. 2-nd layer\n",
    "        ch_layer_3 = config['FIT'].getint('ModelChannelLayer3')                         # channels n. 3-rd layer\n",
    "        ch_reduction = config['FIT'].getint('ModelChannelSpatialReduction')             # ch. n. spatial reduction\n",
    "        kernel_conv1d = config['FIT'].getint('ModelKernelConv1D')                       # conv1d kernel size\n",
    "        kernel_conv2d = tuple([np.int(i) for i in                                       # conv2d kernel size\n",
    "                               config['FIT'].get('ModelKernelConv2D').split(',')])      # .......\n",
    "        dropout_rate = config['FIT'].getfloat('ModelDropout')                           # dropout rate\n",
    "        leaky_slope = config['FIT'].getfloat('ModelLeakySlope')                         # activation fn. slope\n",
    "        learning_rate = config['FIT'].getfloat('ModelLearningRate')                     # learning rate\n",
    "        epochs_n_general = config['FIT'].getint('ModelEpochsNumberGeneral')             # epochs n. general fit\n",
    "        epochs_n_fn = config['FIT'].getint('ModelEpochsNumberFineTuning')               # epochs n. fine tuning\n",
    "        batches_n = config['FIT'].getint('ModelBatchesNumber')                          # batches n.\n",
    "        rnn_cells = [np.int(i) for i in                                                 # rnn cells\n",
    "                     config['FIT'].get('ModelRNNLayers').split(',')]                    # .......\n",
    "        mc_sampling_n = config['FIT'].getint('ModelMCDropoutSamplingNumber')            # sampling n.\n",
    "        ret_sizing = config['FIT'].getfloat('ModelPredictionReturnSizing')              # total return check\n",
    "        ret_adj = config['FIT'].getfloat('ModelPredictionReturnAdj')                    # adjust t+1 day return\n",
    "        s2s_weight = config['FIT'].getfloat('ModelSeq2SeqWeight')                       # seq2seq model weight\n",
    "        am_weight = config['FIT'].getfloat('ModelAlphaWeight')                          # alpha model weight\n",
    "\n",
    "        # D-SAE\n",
    "        dae_learning_rate = config['FIT'].getfloat('DAELearningRate')                   # autoencoder lr.\n",
    "        dae_epochs = config['FIT'].getint('DAEEpochsNumber')                            # autoencoder epochs n.\n",
    "        dae_reg = config['FIT'].getfloat('DAEReg')                                      # autoencoder reg. kernel\n",
    "        dae_batch_n = config['FIT'].getint('DAEBatchesNumber')                          # autoencoder batches n.\n",
    "        dae_layer_1 = config['FIT'].getint('DAENeuronsLayer1')                          # autoencoder neurons n. 1st\n",
    "        dae_features = config['FIT'].getint('DAEFeaturesNumber')                        # autoencoder features\n",
    "\n",
    "        # Tensor\n",
    "        tensor_or_periods = config['TENSOR'].getint('TensorOrTimesteps')                # tensor or. timesteps\n",
    "        tensor_sc_periods = config['TENSOR'].getint('TensorScTimesteps')                # tensor sc. timesteps\n",
    "        tensor_sc_comps = config['TENSOR'].getint('TensorScComponents')                 # tensor sc. components\n",
    "\n",
    "        # Other\n",
    "        adv_period = config['FIT'].getint('ADVPeriod')                                  # adv mean for n. days\n",
    "        scaler_original = config['FIT'].getint('ScalerOriginal')                        # scaler original data\n",
    "        bayesian_ridge_iterations = config['FIT'].getint('BayesianRidgeIter')           # bayesian ridge iter.\n",
    "        gradient_boosting_estimators = config['FIT'].getint('GBoostingEstimators')      # g. boosting estimators\n",
    "        gradient_boosting_lr = config['FIT'].getfloat('GBoostingLr')                    # g. boosting lr.\n",
    "        gradient_boosting_min_imp = config['FIT'].getint('GBoostingMinImportance')      # g. boosting min. imp.\n",
    "\n",
    "        # Transfer learning\n",
    "        activate_tf = config['FIT'].getboolean('ModelActivateTransferLearning')         # transfer learning activation\n",
    "        epochs_n_tf = config['FIT'].getint('ModelEpochsNumberTransferLearning')         # epochs n. transfer lr.\n",
    "        freeze_period = config['FIT'].getint('ModelLayersFreezePeriod')                 # layers freeze period\n",
    "        alpha_freeze = ['dense_8', 'dense_11', 'conv1d_1']                              # freeze layers (conv1d_2?)\n",
    "        beta_freeze = ['dense_17', 'dense_20', 'conv2d_1']                              # freeze layers (conv2d_2?)\n",
    "\n",
    "        # Refit mask. arr.\n",
    "        refit_dates_mask = hl.fn_refit_mask(dates, fit_startdate, refit_freq)\n",
    "        # ---------------------------------\n",
    "        fit_start_idx = np.where(self.data['dates'] > fit_startdate)[0]\n",
    "        assert (fit_start_idx[0] - lookback - 10 > 0), 'Fit start date is not correct, please select later date.'\n",
    "        assert (len(fit_start_idx) > 0), 'Fit start date is not correct, please select earlier date.'\n",
    "\n",
    "        # -------------------------------\n",
    "        #           VARIABLES\n",
    "        # -------------------------------\n",
    "\n",
    "        # TARGET: original returns matrix\n",
    "        ret_mat = hl.handle_returns(self.data['close'], ndays, ret_cap,\n",
    "                                    min_target, industry_balancing, industry)\n",
    "        target_1 = hl.ts_delay(ret_mat, -ndays - delay)\n",
    "        target_2 = hl.ts_delay(ret_mat, -ndays - delay - 1)\n",
    "        target_3 = hl.ts_delay(ret_mat, -ndays - delay - 2)\n",
    "\n",
    "        # TARGET: zscored returns matrix\n",
    "        ret_mat_z = hl.handle_returns_z(self.data['close'], ndays, ret_cap,\n",
    "                                        min_target_z, industry_balancing, industry)\n",
    "        target_z = hl.ts_delay(ret_mat_z, -ndays - delay)\n",
    "\n",
    "        # REGIME: average market return\n",
    "        close = self.data['close']\n",
    "        ret_average = (close - hl.ts_delay(close, ndays)) / hl.ts_delay(close, ndays)\n",
    "        ret_average = np.nanmean(ret_average, axis=0)\n",
    "        regime_mat = np.zeros((close.shape[0], close.shape[1]), dtype=np.float32)\n",
    "        for i in range(regime_mat.shape[1]):\n",
    "            regime_mat[:, i] = ret_average[i]\n",
    "        regime_mat = hl.nan_to_zero(regime_mat)\n",
    "        regime_mat = (regime_mat > 0) * 1.0 + (regime_mat < 0) * (-1.0)\n",
    "\n",
    "        # LIQUIDITY: ranked average trading volume\n",
    "        adv_mat = hl.ts_mean(self.data['close'] * self.data['volume'], adv_period)\n",
    "        adv_mat = hl.cs_rank(hl.zero_to_nan(adv_mat))\n",
    "        adv_mat = hl.nan_to_zero(adv_mat)\n",
    "        adv_mat[adv_mat == 0] = 1\n",
    "\n",
    "        # TENSOR (original): time series data holder.\n",
    "        # available tensors (open, high, low, close)\n",
    "        tensor_fields_original = ['open', 'high', 'low', 'close']\n",
    "        tensor_data_original = np.empty((numstocks, numdates,\n",
    "                                         tensor_or_periods, len(tensor_fields_original)))\n",
    "        for ii, field in enumerate(tensor_fields_original):\n",
    "            with h5py.File('data/pp_data/tensors/' + field + '_original.h5', 'r') as hf:\n",
    "                tensor_data_original[:, :, :, ii] = hf[field + '_original'][:]\n",
    "\n",
    "        # TENSOR (wavelet): time series data holder.\n",
    "        # available tensors (open, high, low, close, adv)\n",
    "        tensor_fields_wavelet = ['open', 'high', 'low', 'close']\n",
    "        tensor_data_wavelet = np.empty((numstocks, numdates,\n",
    "                                        tensor_sc_periods, tensor_sc_comps,\n",
    "                                        len(tensor_fields_wavelet)))\n",
    "        for jj, field in enumerate(tensor_fields_wavelet):\n",
    "            with h5py.File('data/pp_data/tensors/' + field + '_wavelet.h5', 'r') as hf:\n",
    "                tensor_data_wavelet[:, :, :, :, jj] = hf[field + '_wavelet'][:]\n",
    "\n",
    "        # Delete config.\n",
    "        del config\n",
    "\n",
    "        # Data size\n",
    "        data_size = (target_1.nbytes + target_2.nbytes + target_3.nbytes + target_z.nbytes\n",
    "                     + adv_mat.nbytes + tensor_data_original.nbytes + tensor_data_wavelet.nbytes) / (1024 ** 3)\n",
    "        print('Data size: {:.2f}Gb'.format(data_size))\n",
    "        \"\"\"\n",
    "\n",
    "        FUNCTIONS\n",
    "        Functions used to prepare data and train the model.\n",
    "\n",
    "            1.  fe: De-noising stacked autoencoder.\n",
    "            2.  fs: GBoosting based features importance filter.\n",
    "            3.  fe: Stacking Bayesian regression prediction.\n",
    "            4.  nn: Activation function (GeLU).\n",
    "            5.  nn: Loss function.\n",
    "            6.  nn: Loss function (+total return).\n",
    "            7.  nn: Residual block.\n",
    "            8.  nn: Squeeze-and-excitation network.\n",
    "            9.  nn: Feature pyramid network block.\n",
    "            10. nn: Spatial reduction block.\n",
    "            11. ALPHA model.\n",
    "            12. BETA model.\n",
    "\n",
    "        \"\"\"     \n",
    "\n",
    "        # Fn: (5)\n",
    "        # Neural network: loss function.\n",
    "        def nn_loss_sk(y_true, y_pred):\n",
    "            loss = K.switch(K.less(y_true * y_pred, 0),\n",
    "                            loss_adj * y_pred ** 2 - K.sign(y_true) * y_pred + K.abs(y_true),\n",
    "                            K.square(y_pred - y_true))\n",
    "            return K.mean(loss, axis=-1)\n",
    "\n",
    "        # Fn: (6)\n",
    "        # Neural network: loss function (including total return).\n",
    "        def nn_loss_sk_total(y_true, y_pred):\n",
    "            # Day 1 loss.\n",
    "            loss_1 = K.switch(K.less(y_true[:, 0] * y_pred[:, 0], 0),\n",
    "                              loss_adj * y_pred[:, 0] ** 2 - K.sign(y_true[:, 0]) * y_pred[:, 0] + K.abs(y_true[:, 0]),\n",
    "                              K.square(y_pred[:, 0] - y_true[:, 0]))\n",
    "            # Day 2 loss.\n",
    "            loss_2 = K.switch(K.less(y_true[:, 1] * y_pred[:, 1], 0),\n",
    "                              loss_adj * y_pred[:, 1] ** 2 - K.sign(y_true[:, 1]) * y_pred[:, 1] + K.abs(y_true[:, 1]),\n",
    "                              K.square(y_pred[:, 1] - y_true[:, 1]))\n",
    "            # Day 3 loss.\n",
    "            loss_3 = K.switch(K.less(y_true[:, 2] * y_pred[:, 2], 0),\n",
    "                              loss_adj * y_pred[:, 2] ** 2 - K.sign(y_true[:, 2]) * y_pred[:, 2] + K.abs(y_true[:, 2]),\n",
    "                              K.square(y_pred[:, 2] - y_true[:, 2]))\n",
    "            # Total return loss.\n",
    "            loss_4 = K.switch(K.less(y_true[:, 3] * y_pred[:, 3], 0),\n",
    "                              loss_adj * y_pred[:, 3] ** 2 - K.sign(y_true[:, 3]) * y_pred[:, 3] + K.abs(y_true[:, 3]),\n",
    "                              K.square(y_pred[:, 3] - y_true[:, 3]))\n",
    "            # Loss value.\n",
    "            loss = loss_1 * 0.4 + loss_2 * 0.2 + loss_3 * 0.2 + loss_4 * 0.3\n",
    "            return K.mean(loss, axis=-1)\n",
    "    \n",
    "        # Fn: (11)\n",
    "        # First model (alpha) used to predict stock prices.\n",
    "        # 2-rRNet with DeepConv features + Seq2Seq DeepConv-GRU.\n",
    "        # (Fine-tuning 3 lvl)\n",
    "        def nn_model_alpha(alpha_nn, tensor_nn, regime_nn, ones_nn, decoder_nn, target_nn, adv_nn):\n",
    "\n",
    "            # Reshape target value to match ED req.\n",
    "            target_rnn = target_nn.reshape(target_nn.shape[0], target_nn.shape[1], 1)\n",
    "\n",
    "            # Input\n",
    "            input_alpha = Input(shape=(alpha_nn.shape[1],))\n",
    "            input_tensor = Input(shape=(tensor_nn.shape[1], tensor_nn.shape[2]))\n",
    "            input_regime = Input(shape=(regime_nn.shape[1],))\n",
    "            input_ones = Input(shape=(ones_nn.shape[1],))\n",
    "            input_decoder = Input(shape=(None, target_rnn.shape[2]))\n",
    "\n",
    "            # ---------------------------------------------\n",
    "            #             DEEP CONV. NET.\n",
    "            #    (Features for FFN and Input for ED-GRU)\n",
    "            # ---------------------------------------------\n",
    "            # DeepConvNet architecture\n",
    "            conv_node = Conv1D(nb_filter=ch_layer_1, filter_length=kernel_conv1d,\n",
    "                               kernel_regularizer=regularizers.l2(reg_conv))(input_tensor)\n",
    "            conv_node = LeakyReLU(leaky_slope)(conv_node)\n",
    "            conv_node = BatchNormalization()(conv_node)\n",
    "            conv_node = Conv1D(nb_filter=ch_layer_2, filter_length=kernel_conv1d,\n",
    "                               kernel_regularizer=regularizers.l2(reg_conv))(conv_node)\n",
    "            conv_node = LeakyReLU(leaky_slope)(conv_node)\n",
    "            conv_node = BatchNormalization()(conv_node)\n",
    "            conv_node = nn_senet_block(conv_node)                       # Squeeze-and-excitation\n",
    "\n",
    "            # Deep Conv. Net. extracted features.\n",
    "            deep_features = GlobalAveragePooling1D()(conv_node)\n",
    "            deep_features = Dense(deep_features_n)(deep_features)\n",
    "            deep_features = Activation('linear')(deep_features)\n",
    "\n",
    "            # Input to encoder-decoder model.\n",
    "            input_encoder = MaxPooling1D()(conv_node)\n",
    "\n",
    "            # ---------------------------------------------\n",
    "            #              Regime RESNET\n",
    "            # ---------------------------------------------\n",
    "            # Pre-layer: adding deep features to alphas and normalization input.\n",
    "            input_norm = layers.concatenate([input_alpha, deep_features])\n",
    "            input_norm = BatchNormalization()(input_norm)\n",
    "            # 1st node.\n",
    "            res_note = Dense(layer_capacity, kernel_regularizer=regularizers.l2(reg_ffn))(input_norm)\n",
    "            res_note = Dropout(dropout_rate)(res_note, training=True)\n",
    "            res_note = LeakyReLU(leaky_slope)(res_note)\n",
    "            res_note = BatchNormalization()(res_note)\n",
    "            res_note = nn_residual_block(res_note, layer_capacity)      # Residual Block\n",
    "            res_note = Dense(4)(res_note)\n",
    "            res_note = Activation('linear')(res_note)\n",
    "            # 2nd node.\n",
    "            ord_node = Dense(n_layer_1, kernel_regularizer=regularizers.l2(reg_ffn))(input_norm)\n",
    "            ord_node = Dropout(dropout_rate)(ord_node, training=True)\n",
    "            ord_node = LeakyReLU(leaky_slope)(ord_node)\n",
    "            ord_node = BatchNormalization()(ord_node)\n",
    "            ord_node = Dense(n_layer_2, kernel_regularizer=regularizers.l2(reg_ffn))(ord_node)\n",
    "            ord_node = Dropout(dropout_rate)(ord_node, training=True)\n",
    "            ord_node = Activation('gelu')(ord_node)\n",
    "            ord_node = BatchNormalization()(ord_node)\n",
    "            ord_node = Dense(4)(ord_node)\n",
    "            ord_node = Activation('linear')(ord_node)\n",
    "            # Market regimes.\n",
    "            reg_one = Activation('hard_sigmoid')(input_regime)\n",
    "            reg_one = layers.multiply([reg_one, res_note])\n",
    "            reg_two = layers.subtract([input_ones, reg_one])\n",
    "            reg_two = layers.multiply([reg_two, ord_node])\n",
    "            # Union.\n",
    "            reg_out = layers.add([reg_one, reg_two])\n",
    "            reg_out = Dense(4, activation='linear')(reg_out)\n",
    "\n",
    "            # ---------------------------------------------\n",
    "            #             ENCODER-DECODER GRU\n",
    "            # ---------------------------------------------\n",
    "            # Encoder architecture\n",
    "            encoder_cells = []\n",
    "            for hidden_neurons in rnn_cells:\n",
    "                encoder_cells.append(GRUCell(hidden_neurons,\n",
    "                                             kernel_regularizer=regularizers.l2(reg_gru)))\n",
    "            encoder = RNN(encoder_cells, return_state=True)\n",
    "            encoder_outputs_and_states = encoder(input_encoder)\n",
    "            encoder_states = encoder_outputs_and_states[1:]\n",
    "            # Decoder architecture\n",
    "            decoder_cells = []\n",
    "            for hidden_neurons in rnn_cells:\n",
    "                decoder_cells.append(GRUCell(hidden_neurons,\n",
    "                                             kernel_regularizer=regularizers.l2(reg_gru)))\n",
    "            decoder = RNN(decoder_cells, return_sequences=True, return_state=True)\n",
    "            decoder_outputs_and_states = decoder(input_decoder, initial_state=encoder_states)\n",
    "            # Out\n",
    "            decoder_outputs = decoder_outputs_and_states[0]\n",
    "            decoder_outputs = Dense(1, activation='linear')(decoder_outputs)\n",
    "\n",
    "            # Output layers\n",
    "            out_single = reg_out\n",
    "            out_multi = decoder_outputs\n",
    "\n",
    "            # Optimizer.\n",
    "            adam_opt = optimizers.Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "\n",
    "            # Model final in-out structure.\n",
    "            model = Model(inputs=[input_alpha, input_tensor, input_regime, input_ones, input_decoder],\n",
    "                          outputs=[out_single, out_multi])\n",
    "            model.compile(optimizer=adam_opt,\n",
    "                          loss=[nn_loss_sk_total, nn_loss_sk],\n",
    "                          loss_weights=[1 - s2s_weight, s2s_weight])\n",
    "            # model.summary()\n",
    "\n",
    "            # Train the model.\n",
    "            print('      training neural network - ALPHA')\n",
    "\n",
    "            # GENERAL FIT <1st>\n",
    "            # Fit works with all input data.\n",
    "            model.fit(x=[alpha_nn, tensor_nn, regime_nn, ones_nn, decoder_nn],\n",
    "                      y=[target_nn, target_rnn[:, :-1]],\n",
    "                      epochs=epochs_n_general, batch_size=int(alpha_nn.shape[0] / batches_n),\n",
    "                      shuffle=False, verbose=2)\n",
    "\n",
    "            # FINE-TUNING <2nd>\n",
    "            # Fit takes as input only the latest values trying to capture\n",
    "            # the most relevant market information.\n",
    "            latest_index = int(target_nn.shape[0] * 0.1)\n",
    "            model.fit(x=[alpha_nn[latest_index:], tensor_nn[latest_index:],\n",
    "                         regime_nn[latest_index:], ones_nn[latest_index:],\n",
    "                         decoder_nn[latest_index:]],\n",
    "                      y=[target_nn[latest_index:], target_rnn[latest_index:, :-1]],\n",
    "                      epochs=epochs_n_fn, batch_size=int(alpha_nn.shape[0] / batches_n),\n",
    "                      shuffle=False, verbose=2)\n",
    "\n",
    "            # FINE-TUNING <3rd>\n",
    "            # Fit takes as input only the most liquid stocks trying\n",
    "            # to focus only on predicting them.\n",
    "            liquid_idx = (adv_nn > 1.25)\n",
    "            model.fit(x=[alpha_nn[liquid_idx], tensor_nn[liquid_idx],\n",
    "                         regime_nn[liquid_idx], ones_nn[liquid_idx],\n",
    "                         decoder_nn[liquid_idx]],\n",
    "                      y=[target_nn[liquid_idx], target_rnn[liquid_idx, :-1]],\n",
    "                      epochs=epochs_n_fn, batch_size=int(alpha_nn.shape[0] / batches_n),\n",
    "                      shuffle=False, verbose=2)\n",
    "\n",
    "            # FINE-TUNING <4th>\n",
    "            # Fit takes model predictions and use only bad predicted data points.\n",
    "            # --> first let`s take model predictions and rank them.\n",
    "            model_prediction = model.predict([alpha_nn, tensor_nn, regime_nn, ones_nn, decoder_nn])\n",
    "            # 2-RRNet predictions handling.\n",
    "            pred_ffn = model_prediction[0]\n",
    "            pred_ffn = pred_ffn / scaler_original\n",
    "            pred_ffn = hl.nn_handle_multi_day_predictions(pred_ffn, ret_sizing, ret_adj)\n",
    "            # S2S-DeepConv-GRU predictions handling.\n",
    "            pred_en = model_prediction[1].reshape(model_prediction[1].shape[0], model_prediction[1].shape[1])\n",
    "            pred_en = pred_en / scaler_original\n",
    "            pred_en = hl.nn_handle_multi_day_predictions(pred_en, ret_sizing, ret_adj)\n",
    "            # Final prediction.\n",
    "            prediction_internal = hl.cs_zscore(pred_ffn) * (1 - s2s_weight) + hl.cs_zscore(pred_en) * s2s_weight\n",
    "            # --> now define pnl var.\n",
    "            pnl_predicted = prediction_internal * target_nn[:, 0]\n",
    "            pnl_predicted = hl.nan_to_zero(hl.cs_rank(pnl_predicted))\n",
    "            pnl_predicted[pnl_predicted == 0] = 1\n",
    "            pnl_predicted_idx = (pnl_predicted < 1.5)\n",
    "            # --> finally fit the model.\n",
    "            model.fit(x=[alpha_nn[pnl_predicted_idx], tensor_nn[pnl_predicted_idx],\n",
    "                         regime_nn[pnl_predicted_idx], ones_nn[pnl_predicted_idx],\n",
    "                         decoder_nn[pnl_predicted_idx]],\n",
    "                      y=[target_nn[pnl_predicted_idx], target_rnn[pnl_predicted_idx, :-1]],\n",
    "                      epochs=epochs_n_fn, batch_size=int(alpha_nn.shape[0] / batches_n),\n",
    "                      shuffle=False, verbose=2)\n",
    "\n",
    "            return model\n",
    "       \n",
    "\n",
    "        \"\"\"        \n",
    "        ----------------------------------------------------------------------------------------------------------\n",
    "                                                        MODEL\n",
    "                                    Training loop goes through each refit date\n",
    "                                        to fit the model on historical data.\n",
    "        ----------------------------------------------------------------------------------------------------------\n",
    "        \"\"\"\n",
    "     \n",
    "        # Define dictionary to store trained models.\n",
    "        model_dict = dict()\n",
    "\n",
    "        # Walk through each day.\n",
    "        for di, date in enumerate(dates):\n",
    "            if di <= lookback or date <= fit_startdate:\n",
    "                continue\n",
    "\n",
    "            # Fit the model.\n",
    "            if refit_dates_mask[di]:\n",
    "                print('')\n",
    "                print('  Fitting on {}'.format(date))\n",
    "\n",
    "                # Start and end indices\n",
    "                idx_start = di - delay - lookback\n",
    "                idx_end = di - delay - 1\n",
    "\n",
    "                # PREPROCESS DATA.\n",
    "                # ---------------------------------------------\n",
    "                # Target 1st day.\n",
    "                target_reg = np.copy(target_1[:, idx_start: idx_end])\n",
    "                target_reg[:, -1 - delay:] = np.nan\n",
    "                target_reg = target_reg.reshape(-1, order='F')\n",
    "                target_valids = np.isfinite(target_reg)\n",
    "                target_reg = target_reg[target_valids]\n",
    "                # Target 2nd day.\n",
    "                target_2_reg = np.copy(target_2[:, idx_start: idx_end])\n",
    "                target_2_reg[:, -1 - delay:] = np.nan\n",
    "                target_2_reg = target_2_reg.reshape(-1, order='F')\n",
    "                target_2_reg = target_2_reg[target_valids]\n",
    "                # Target 3rd day.\n",
    "                target_3_reg = np.copy(target_3[:, idx_start: idx_end])\n",
    "                target_3_reg[:, -1 - delay:] = np.nan\n",
    "                target_3_reg = target_3_reg.reshape(-1, order='F')\n",
    "                target_3_reg = target_3_reg[target_valids]\n",
    "                # Target (Zscore)\n",
    "                target_reg_z = np.copy(target_z[:, idx_start: idx_end])\n",
    "                target_reg_z[:, -1 - delay:] = np.nan\n",
    "                target_reg_z = target_reg_z.reshape(-1, order='F')\n",
    "                target_reg_z = target_reg_z[target_valids]\n",
    "\n",
    "                # Tensor original\n",
    "                or_reg = np.copy(tensor_data_original[:, idx_start: idx_end])\n",
    "                or_reg[:, -1 - delay:] = np.nan\n",
    "                or_reg = or_reg.reshape(or_reg.shape[0] * or_reg.shape[1],\n",
    "                                        tensor_or_periods, len(tensor_fields_original), order='F')\n",
    "                or_reg = or_reg[target_valids]\n",
    "\n",
    "                # Tensor scalogram\n",
    "                sc_reg = np.copy(tensor_data_wavelet[:, idx_start: idx_end])\n",
    "                sc_reg[:, -1 - delay:] = np.nan\n",
    "                sc_reg = sc_reg.reshape(sc_reg.shape[0] * sc_reg.shape[1], tensor_sc_periods,\n",
    "                                        tensor_sc_comps, len(tensor_fields_wavelet), order='F')\n",
    "                sc_reg = sc_reg[target_valids]\n",
    "\n",
    "                # Regime var.\n",
    "                regime_var = np.copy(regime_mat[:, idx_start: idx_end])\n",
    "                regime_var[:, -1 - delay:] = np.nan\n",
    "                regime_var_di = regime_var.reshape(-1, order='F')\n",
    "                regime_reg = regime_var_di[target_valids]\n",
    "                # Liquidity var.\n",
    "                adv_var = np.copy(adv_mat[:, idx_start: idx_end])\n",
    "                adv_var[:, -1 - delay:] = np.nan\n",
    "                adv_var_di = adv_var.reshape(-1, order='F')\n",
    "                adv_reg = adv_var_di[target_valids]\n",
    "                # -----------------------------------------------\n",
    "\n",
    "                # Load alphas.\n",
    "                alpha_reg = np.empty((target_reg.shape[0], len(alphas_list)), dtype=np.float64, order='F')\n",
    "                for i, alpha_name in enumerate(alphas_list):\n",
    "                    temp_alpha = joblib.load(self.path_alphas + alpha_name)\n",
    "                    alpha = np.copy(temp_alpha[:, idx_start: idx_end])\n",
    "                    alpha = hl.zero_to_nan(alpha)\n",
    "                    alpha = hl.nan_to_zero(alpha.reshape(-1, order='F')[target_valids])\n",
    "                    alpha_reg[:, i] = alpha\n",
    "                # Load events.\n",
    "                event_reg = np.empty((target_reg.shape[0], len(events_list)), dtype=np.int32, order='F')\n",
    "                for i, event_name in enumerate(events_list):\n",
    "                    temp_event = joblib.load(self.path_events + event_name)\n",
    "                    event = np.copy(temp_event[:, idx_start: idx_end])\n",
    "                    event = hl.zero_to_nan(event)\n",
    "                    event = hl.nan_to_zero(event.reshape(-1, order='F')[target_valids])\n",
    "                    event_reg[:, i] = event\n",
    "\n",
    "                # Cap alphas min/max values and unite.\n",
    "                alpha_reg[alpha_reg > max_alpha] = max_alpha\n",
    "                alpha_reg[alpha_reg < -max_alpha] = -max_alpha\n",
    "                alpha_reg[np.abs(alpha_reg) < min_alpha] = 0\n",
    "                alpha_reg = hl.nan_to_zero(alpha_reg)\n",
    "                target_reg = hl.nan_to_zero(target_reg)\n",
    "\n",
    "                # Break condition\n",
    "                if alpha_reg.size == 0 or target_reg.size == 0:\n",
    "                    continue\n",
    "                assert alpha_reg.shape[0] == target_reg.shape[0], 'Shapes doesn`t match'\n",
    "                assert or_reg.shape[0] == target_reg.shape[0], 'Shapes doesn`t match'\n",
    "                assert sc_reg.shape[0] == target_reg.shape[0], 'Shapes doesn`t match'\n",
    "\n",
    "                print('      vars. loaded')\n",
    "\n",
    "                \"\"\"                \n",
    "                    Pre-model.   \n",
    "                    Brief description:\n",
    "                        - Prepare data to feed the model.   \n",
    "                    Structure:\n",
    "                        - Feature Eng: D-SAE model extracts high level deep features from given alpha matrix.\n",
    "                        - Feature selection: GBoosting based feature importance filter drops irrelevant features.\n",
    "                        - Stacking (alpha): add events to alpha matrix.\n",
    "                        - Stacking (alpha): Bayesian Ridge model predicts target values and stack \n",
    "                        predictions to alpha matrix. \n",
    "                        - Stacking (target): add two more days to target matrix (original).\n",
    "                        - Stacking (target): add total return (3 days) to target values (original).\n",
    "\n",
    "                \"\"\"\n",
    "               \n",
    "                # Stacking events:\n",
    "                #     - EPS data.\n",
    "                #     - DIV data.\n",
    "                alpha_reg = np.hstack((alpha_reg, event_reg))\n",
    "\n",
    "                # Stack multiple predictions (3 days)\n",
    "                target_reg = np.hstack((target_reg.reshape(-1, 1), target_2_reg.reshape(-1, 1)))\n",
    "                target_reg = np.hstack((target_reg, target_3_reg.reshape(-1, 1)))\n",
    "                \n",
    "                # Scale original target and tensor values.\n",
    "                target_reg = target_reg * scaler_original\n",
    "                or_reg = or_reg * scaler_original\n",
    "\n",
    "                # Final check\n",
    "                target_reg = hl.nan_to_zero(target_reg)\n",
    "                decoder_reg = np.zeros((alpha_reg.shape[0], 3, 1))\n",
    "\n",
    "                break\n",
    "            \n",
    "        return target_reg, or_reg, decoder_reg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fit model:\n",
      "Days available: 2037\n",
      "Data size: 1.93Gb\n",
      "\n",
      "  Fitting on 20160104\n",
      "      vars. loaded\n",
      "(82651, 3)\n",
      "(82651, 20, 4)\n",
      "(82651, 3, 1)\n"
     ]
    }
   ],
   "source": [
    "target, tensor, dec = Fit().run_model()\n",
    "\n",
    "print(target.shape)\n",
    "print(tensor.shape)\n",
    "print(dec.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_reg = np.copy(tensor)\n",
    "ret_reg = np.copy(target)\n",
    "dec_reg = np.copy(dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_36 (InputLayer)        (None, 20, 4)             0         \n",
      "_________________________________________________________________\n",
      "gru_17 (GRU)                 (None, 10)                450       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 461\n",
      "Trainable params: 461\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "done defining network\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1fd5f626cc0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.keras.layers import Activation\n",
    "\n",
    "n_batches = 200\n",
    "days_seq = 3\n",
    "learning_rate = 0.002\n",
    "adam_lr_decay = 0.005\n",
    "rnn_neurons_1 = 10\n",
    "reg_gru = 0.01\n",
    "n_epochs = 4\n",
    "numstocks = 599\n",
    "\n",
    "\n",
    " # Select relevant days to predict.\n",
    "ret_reg = ret_reg[:, :1]\n",
    "# ------------------\n",
    "\n",
    "# Inputs\n",
    "input_tensor = Input(shape=(tensor_reg.shape[1], tensor_reg.shape[2]))\n",
    "input_encoder = input_tensor\n",
    "\n",
    "# ---------------------------------------------\n",
    "#                   LSTM\n",
    "# ---------------------------------------------\n",
    "node_lstm = GRU(rnn_neurons_1, return_sequences=True)(input_encoder)\n",
    "node_lstm = Activation('tanh')(node_lstm)\n",
    "node_lstm = GRU(rnn_neurons_1, return_sequences=False)(input_encoder)\n",
    "node_lstm = Activation('tanh')(node_lstm)\n",
    "node_lstm = Dense(1, activation='linear')(node_lstm)\n",
    "\n",
    "# Output layers\n",
    "out_gru = node_lstm\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs=[input_tensor],\n",
    "              outputs=[out_gru])\n",
    "model.compile(optimizer='adam',\n",
    "              loss=['mse'])\n",
    "model.summary()\n",
    "print('done defining network')\n",
    "\n",
    "# Train second model\n",
    "median_data_index = int(ret_reg.shape[0] / 2)\n",
    "batch_size = int(ret_reg.shape[0] / n_batches)\n",
    "\n",
    "model.fit(x=[tensor_reg],\n",
    "          y=[ret_reg],\n",
    "          epochs=n_epochs - 2,\n",
    "          batch_size=batch_size,\n",
    "          shuffle=True, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = tensor_reg[-1000:]\n",
    "predicted_returns = model.predict([tensor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(predicted_returns.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (10000, 20, 4)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (10000, None, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gru_4 (GRU)                     [(10000, 20, 30), (1 3150        input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "gru_5 (GRU)                     [(10000, None, 30),  2880        input_8[0][0]                    \n",
      "                                                                 gru_4[0][1]                      \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer_2 (AttentionLay [(10000, None, 30),  1830        gru_4[0][0]                      \n",
      "                                                                 gru_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (10000, None, 60)    0           gru_5[0][0]                      \n",
      "                                                                 attention_layer_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_4 (TimeDistrib (10000, None, 1)     61          concatenate_2[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 7,921\n",
      "Trainable params: 7,921\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "10000/10000 [==============================] - 1s 69us/sample - loss: 1.2061\n",
      "10000/10000 [==============================] - 0s 9us/sample - loss: 1.3435\n",
      "10000/10000 [==============================] - 0s 9us/sample - loss: 0.9226\n",
      "10000/10000 [==============================] - 0s 10us/sample - loss: 1.5427\n",
      "10000/10000 [==============================] - 0s 9us/sample - loss: 1.5357\n",
      "10000/10000 [==============================] - 0s 9us/sample - loss: 1.6831\n",
      "10000/10000 [==============================] - 0s 9us/sample - loss: 2.4075\n",
      "10000/10000 [==============================] - 0s 9us/sample - loss: 1.9544\n",
      "Loss in epoch 1: 1.5744476616382599\n",
      "10000/10000 [==============================] - 0s 9us/sample - loss: 1.2007\n",
      "10000/10000 [==============================] - 0s 9us/sample - loss: 1.3425\n",
      "10000/10000 [==============================] - 0s 9us/sample - loss: 0.9229\n",
      "10000/10000 [==============================] - 0s 11us/sample - loss: 1.5462\n",
      "10000/10000 [==============================] - 0s 10us/sample - loss: 1.5340\n",
      "10000/10000 [==============================] - 0s 11us/sample - loss: 1.6807\n",
      "10000/10000 [==============================] - 0s 9us/sample - loss: 2.4056\n",
      "10000/10000 [==============================] - 0s 9us/sample - loss: 1.9554\n",
      "Loss in epoch 2: 1.5734989941120148\n",
      "10000/10000 [==============================] - 0s 9us/sample - loss: 1.1983\n",
      "10000/10000 [==============================] - 0s 9us/sample - loss: 1.3418\n",
      "10000/10000 [==============================] - 0s 10us/sample - loss: 0.9222\n",
      "10000/10000 [==============================] - 0s 9us/sample - loss: 1.5471\n",
      "10000/10000 [==============================] - 0s 9us/sample - loss: 1.5333\n",
      "10000/10000 [==============================] - 0s 9us/sample - loss: 1.6796\n",
      "10000/10000 [==============================] - 0s 9us/sample - loss: 2.4048\n",
      "10000/10000 [==============================] - 0s 9us/sample - loss: 1.9554\n",
      "Loss in epoch 3: 1.5728190243244171\n",
      "10000/10000 [==============================] - 0s 9us/sample - loss: 1.1980\n",
      "10000/10000 [==============================] - 0s 10us/sample - loss: 1.3417\n",
      "10000/10000 [==============================] - 0s 9us/sample - loss: 0.9222\n",
      "10000/10000 [==============================] - 0s 9us/sample - loss: 1.5470\n",
      "10000/10000 [==============================] - 0s 9us/sample - loss: 1.5325\n",
      "10000/10000 [==============================] - 0s 9us/sample - loss: 1.6789\n",
      "10000/10000 [==============================] - 0s 10us/sample - loss: 2.4040\n",
      "10000/10000 [==============================] - 0s 9us/sample - loss: 1.9547\n",
      "Loss in epoch 4: 1.572380393743515\n"
     ]
    }
   ],
   "source": [
    "# Params\n",
    "days_seq = 3\n",
    "n_epochs = 4\n",
    "batch_size = 10000\n",
    "\n",
    "# Define decoder dummy.\n",
    "decoder_reg = np.zeros((ret_reg.shape[0], days_seq, 1))\n",
    "\n",
    "# Reshape target value to match ED req.\n",
    "target_rnn = ret_reg.reshape(ret_reg.shape[0], ret_reg.shape[1], 1)\n",
    "\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(batch_shape=(batch_size, tensor_reg.shape[1], tensor_reg.shape[2]))\n",
    "decoder_inputs = Input(batch_shape=(batch_size, None, target_rnn.shape[2]))\n",
    "\n",
    "# Encoder GRU\n",
    "encoder_gru = GRU(30, return_sequences=True, return_state=True)\n",
    "encoder_out, encoder_state = encoder_gru(encoder_inputs)\n",
    "\n",
    "# Set up the decoder GRU, using `encoder_states` as initial state.\n",
    "decoder_gru = GRU(30, return_sequences=True, return_state=True)\n",
    "decoder_out, decoder_state = decoder_gru(decoder_inputs, initial_state=encoder_state)\n",
    "\n",
    "# Attention layer\n",
    "attn_layer = AttentionLayer()\n",
    "attn_out, attn_states = attn_layer([encoder_out, decoder_out])\n",
    "\n",
    "# Concat attention input and decoder GRU output\n",
    "decoder_concat_input = Concatenate(axis=-1)([decoder_out, attn_out])\n",
    "\n",
    "# Dense layer\n",
    "dense = Dense(target_rnn.shape[2], activation='linear')\n",
    "dense_time = TimeDistributed(dense)\n",
    "decoder_pred = dense_time(decoder_concat_input)\n",
    "\n",
    "# Full model\n",
    "full_model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_pred)\n",
    "full_model.compile(optimizer='adam', loss='mse')\n",
    "full_model.summary()\n",
    "\n",
    "for ep in range(n_epochs):\n",
    "    losses = []    \n",
    "    for bi in range(0, target_rnn.shape[0] - batch_size, batch_size):\n",
    "        tensor_seq = tensor_reg[bi:bi + batch_size]\n",
    "        dec_seq = dec_reg[bi:bi + batch_size]\n",
    "        target_seq = target_rnn[bi:bi + batch_size]        \n",
    "        full_model.train_on_batch(x=[tensor_seq, dec_seq], \n",
    "                             y=[target_seq])\n",
    "        l = full_model.evaluate(x=[tensor_seq, dec_seq], \n",
    "                           y=[target_seq],\n",
    "                           batch_size=batch_size, \n",
    "                           verbose=1)\n",
    "        losses.append(l)\n",
    "    if (ep + 1) % 1 == 0:\n",
    "        print(\"Loss in epoch {}: {}\".format(ep + 1, np.mean(losses)))\n",
    "\n",
    "        \n",
    "\"\"\" Inference model \"\"\"\n",
    "batch_size = 32\n",
    "\n",
    "\"\"\" Encoder (Inference) model \"\"\"\n",
    "encoder_inf_inputs = Input(batch_shape=(batch_size, tensor_reg.shape[1], tensor_reg.shape[2]))\n",
    "encoder_inf_out, encoder_inf_state = encoder_gru(encoder_inf_inputs)\n",
    "encoder_model = Model(inputs=encoder_inf_inputs, outputs=[encoder_inf_out, encoder_inf_state])\n",
    "\n",
    "\"\"\" Decoder (Inference) model \"\"\"\n",
    "decoder_inf_inputs = Input(batch_shape=(batch_size, 3, target_rnn.shape[2]))\n",
    "encoder_inf_states = Input(batch_shape=(batch_size, tensor_reg.shape[1], 30))\n",
    "decoder_init_state = Input(batch_shape=(batch_size, 30))\n",
    "\n",
    "decoder_inf_out, decoder_inf_state = decoder_gru(decoder_inf_inputs, initial_state=decoder_init_state)\n",
    "attn_inf_out, attn_inf_states = attn_layer([encoder_inf_states, decoder_inf_out])\n",
    "decoder_inf_concat = Concatenate(axis=-1)([decoder_inf_out, attn_inf_out])\n",
    "decoder_inf_pred = TimeDistributed(dense)(decoder_inf_concat)\n",
    "decoder_model = Model(inputs=[encoder_inf_states, decoder_init_state, decoder_inf_inputs],\n",
    "                      outputs=[decoder_inf_pred, attn_inf_states, decoder_inf_state])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 32\n",
    "ret = ret_reg[-idx:]\n",
    "tensor = tensor_reg[-idx:]\n",
    "dec = dec_reg[-idx:]\n",
    "enc_outs, enc_last_state = encoder_model.predict(tensor)\n",
    "dec_state = enc_last_state\n",
    "dec_out, attention, dec_state = decoder_model.predict([enc_outs, dec_state, dec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
